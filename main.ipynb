{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = [4,4,3]\n",
    "dataset = \"iris\" # \"iris\", \"autoasocjacja\"\n",
    "starting_bias = 0\n",
    "\n",
    "# ile procent danych ma byc w zbiorze uczacym\n",
    "# ignorowane dla autoasocjacji\n",
    "dataset_split_ratio = 0.5 \n",
    "lr = 0.1\n",
    "momentum = 0.0\n",
    "epochs = 2000\n",
    "target_error = None # float or None\n",
    "shuffle = True\n",
    "input_weights_filename = \"weights.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "output_dir = f\"{dataset}\"\n",
    "output_dir += f\"_sbias{starting_bias}\"\n",
    "if dataset != \"autoasocjacja\":\n",
    "    output_dir += f\"_split{int(dataset_split_ratio*100)}\"\n",
    "output_dir += f\"_lr{lr}\"\n",
    "output_dir += f\"_m{momentum}\"\n",
    "output_dir += f\"_e{epochs}\"\n",
    "output_dir += f\"_et{target_error}\"\n",
    "output_dir += f\"_shuffle{int(shuffle)}\"\n",
    "if os.path.exists(output_dir):\n",
    "    os.rmdir(output_dir) \n",
    "os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = None\n",
    "try:\n",
    "    weights = np.load(input_weights_filename+\".npy\")\n",
    "except:\n",
    "    weights = np.zeros((len(neurons)-1, max(neurons), max(neurons)+1))\n",
    "    for i in range(len(weights)):\n",
    "        layer = np.random.rand(neurons[i+1], neurons[i]+1)\n",
    "        layer[:,0] = starting_bias # bias for that layer\n",
    "        layer.resize(max(neurons), max(neurons)+1)\n",
    "        weights[i] = layer\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = None\n",
    "testing_data = None\n",
    "iris = None\n",
    "if dataset == \"iris\":\n",
    "    from sklearn import datasets, preprocessing\n",
    "    iris = datasets.load_iris()\n",
    "    data = preprocessing.normalize(iris.data)\n",
    "    one_hot_targets = np.zeros((len(data), np.max(iris.target)+1))\n",
    "    for i in range(len(data)):\n",
    "        one_hot_targets[i,iris.target[i]] = 1\n",
    "    data = [row for row in data]\n",
    "    targets = [row for row in one_hot_targets]\n",
    "    whole_data = list(zip(data, targets))\n",
    "    if shuffle:\n",
    "        random.shuffle(whole_data)\n",
    "    training_data = whole_data[:int(len(whole_data)*dataset_split_ratio)]\n",
    "    testing_data = whole_data[int(len(whole_data)*dataset_split_ratio):]\n",
    "elif dataset == \"autoasocjacja\":\n",
    "    training_data = [\n",
    "        (np.array([1,0,0,0]), np.array([1,0,0,0])),\n",
    "        (np.array([0,1,0,0]), np.array([0,1,0,0])),\n",
    "        (np.array([0,0,1,0]), np.array([0,0,1,0])),\n",
    "        (np.array([0,0,0,1]), np.array([0,0,0,1])),\n",
    "    ]\n",
    "    testing_data = [\n",
    "        (np.array([1,0,0,0]), np.array([1,0,0,0])),\n",
    "        (np.array([0,1,0,0]), np.array([0,1,0,0])),\n",
    "        (np.array([0,0,1,0]), np.array([0,0,1,0])),\n",
    "        (np.array([0,0,0,1]), np.array([0,0,0,1])),\n",
    "    ]\n",
    "else:\n",
    "    raise ValueError(\"Unknown dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return (1 / (1+np.pow(np.e, -x)))\n",
    "def activation_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference\n",
    "def inference(weights, data, inputs=None, outputs=None):\n",
    "    sample = np.copy(data)\n",
    "    sample.resize(max(neurons))\n",
    "\n",
    "    # add 1 at the beginning so bias properly applies \n",
    "    sample = np.pad(sample, (1,0), \"constant\", constant_values=(1))\n",
    "    for layer in weights:\n",
    "        if inputs is not None:\n",
    "            inputs.append(np.copy(sample))\n",
    "        sample = layer @ sample\n",
    "        sample = activation(sample)\n",
    "        if outputs is not None:\n",
    "            outputs.append(np.copy(sample))\n",
    "        sample = np.pad(sample, (1,0), \"constant\", constant_values=(1))\n",
    "    return sample[1:(neurons[-1]+1)] # strip all the useless padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_deltas(output, target, weights, activations):\n",
    "    delta = (output - target) * activation_derivative(output)\n",
    "    delta.resize(max(neurons))\n",
    "    deltas = [delta]\n",
    "\n",
    "    # Backward pass\n",
    "    for i in range(len(weights)-1, 0, -1):\n",
    "        act = activations[i-1][0:neurons[i]]\n",
    "        w = weights[i][:,1:neurons[i]+1]\n",
    "        delta = (w.T @ deltas[0]) * activation_derivative(act)\n",
    "        delta.resize(max(neurons))\n",
    "        deltas.insert(0, delta)\n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(weights, sample, target, lr=0.1, momentum=0.0, prev_updates=None):\n",
    "    # Forward pass\n",
    "    activations = []\n",
    "    inputs = []\n",
    "    output = inference(weights, sample, inputs, activations)\n",
    "    # Calculate output error\n",
    "    error = np.mean((output - target) ** 2)\n",
    "\n",
    "    deltas = calc_deltas(output, target, weights, activations)\n",
    "\n",
    "    # Initialize previous updates if needed\n",
    "    if prev_updates is None:\n",
    "        prev_updates = [np.zeros_like(w) for w in weights]\n",
    "\n",
    "    # Update weights with momentum\n",
    "    new_prev_updates = []\n",
    "    for i in range(len(weights)):\n",
    "        inp = inputs[i][np.newaxis, :]\n",
    "        delt = deltas[i][:, np.newaxis]\n",
    "        update = lr * delt @ inp\n",
    "        weights[i][:, :inp.shape[1]] -= update + momentum * prev_updates[i][:, :inp.shape[1]]\n",
    "        # Store this update for next iteration\n",
    "        prev_update = update\n",
    "        \n",
    "        if starting_bias == 0:\n",
    "            # Remove bias from the updates and weights\n",
    "            prev_update[:, 0] = 0\n",
    "            weights[i][:, 0] = 0\n",
    "\n",
    "        new_prev_updates.append(prev_update)\n",
    "\n",
    "    return (weights, error, new_prev_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "prev_updates = None\n",
    "for epoch in range(epochs):\n",
    "    epochError = []\n",
    "    if shuffle:\n",
    "        random.shuffle(training_data)\n",
    "    for sample, target in training_data:\n",
    "        (weights, error, prev_updates) = backprop(weights, sample, target, lr=lr, momentum=momentum, prev_updates=prev_updates)\n",
    "        epochError.append(error)\n",
    "    errors.append(np.mean(epochError))\n",
    "    if target_error is not None and errors[-1] <= target_error:\n",
    "        print(f\"Target error reached at epoch {epoch}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir,\"globalErrors.log\"), \"wb\") as f:\n",
    "    f.write(\"\\n\".join([str(e) for e in errors]).encode(\"utf-8\"))\n",
    "plt.figure()\n",
    "plt.plot(errors)\n",
    "plt.title(\"Error over time\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(output_dir,\"globalErrors.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(output_dir,\"weights.npy\"), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_and_print(string, file):\n",
    "    print(string)\n",
    "    file.write(str(string) + \"\\n\")\n",
    "fileOutputs = []\n",
    "with open(os.path.join(output_dir,\"summary.log\"), \"w\") as summary:\n",
    "    if(dataset == \"iris\"):\n",
    "        outputs = []\n",
    "        targets = []\n",
    "        for sample, target in testing_data:\n",
    "            fileOutput = {}\n",
    "            fileOutput[\"in\"] = sample.tolist()\n",
    "            activations = []\n",
    "            inferenceOut = inference(weights, sample, None, activations)\n",
    "            fileOutput[\"error\"] = np.mean((inferenceOut - target) ** 2)\n",
    "            fileOutput[\"target\"] = target.tolist()\n",
    "            fileOutput[\"deltas\"] = [d.tolist() for d in calc_deltas(inferenceOut, target, weights, activations)]\n",
    "            fileOutput[\"weights\"] = np.flip(weights,0).tolist()\n",
    "            fileOutput[\"activations\"] = [d.tolist() for d in list(reversed(activations))]\n",
    "            fileOutputs.append(fileOutput)\n",
    "            \n",
    "            outputs.append(np.argmax(inference(weights, sample)))\n",
    "            targets.append(np.argmax(target))\n",
    "        \n",
    "        write_and_print(\"Macierz pomyłek:\", summary)\n",
    "        cmatrix = confusion_matrix(targets, outputs)\n",
    "        df = pd.DataFrame(cmatrix, index=iris.target_names, columns=iris.target_names)\n",
    "        write_and_print(df, summary)\n",
    "        write_and_print(\"\", summary)\n",
    "        write_and_print(\n",
    "            pd.DataFrame(\n",
    "                cmatrix.diagonal(), \n",
    "                index=iris.target_names,\n",
    "                columns=[\"Poprawne klasyfikacje\"],\n",
    "            ), summary\n",
    "        )\n",
    "        write_and_print(\"\", summary)\n",
    "        diagonal_sum = cmatrix.diagonal().sum()\n",
    "        matrix_all = cmatrix.sum()\n",
    "        percent = round(diagonal_sum / matrix_all * 100, 2)\n",
    "        write_and_print(f\"Poprawnie sklasyfikowano: {diagonal_sum}/{matrix_all} {percent}%\", summary)\n",
    "        write_and_print(\"\", summary)\n",
    "        if iris is not None:\n",
    "            write_and_print(\"Classification report:\", summary)\n",
    "            write_and_print(classification_report(targets, outputs, target_names=iris.target_names), summary)\n",
    "    elif dataset == \"autoasocjacja\":\n",
    "        write_and_print(f\"Liczba epok: {len(errors)} \\n\", summary)\n",
    "        for sample, target in testing_data:\n",
    "            fileOutput = {}\n",
    "            fileOutput[\"in\"] = sample.tolist()\n",
    "            activations = []\n",
    "            inferenceOut = inference(weights, sample, None, activations)\n",
    "            fileOutput[\"error\"] = np.mean((inferenceOut - target) ** 2)\n",
    "            fileOutput[\"target\"] = target.tolist()\n",
    "            fileOutput[\"deltas\"] = [d.tolist() for d in calc_deltas(inferenceOut, target, weights, activations)]\n",
    "            fileOutput[\"weights\"] = np.flip(weights,0).tolist()\n",
    "            fileOutput[\"activations\"] = [d.tolist() for d in list(reversed(activations))]\n",
    "            fileOutputs.append(fileOutput)\n",
    "            write_and_print(f\"{sample} => {np.array2string(inferenceOut, precision=8, suppress_small=True)}\", summary)\n",
    "    json.dump(fileOutputs, open(os.path.join(output_dir,\"inference.log.json\"), \"w\"), indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
